"""
–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è AI Resume Assistant

–ï–¥–∏–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å–æ –≤—Å–µ–º–∏ 4 —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
- Gap-–∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞
- –ß–µ–∫-–ª–∏—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∏–Ω—Ç–µ—Ä–≤—å—é
- –°–∏–º—É–ª—è—Ü–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é

–ó–∞–ø—É—Å–∫: python -m src.web_app.unified_app.main
URL: http://localhost:3000
"""

import os
import tempfile
import asyncio
from pathlib import Path
from fastapi import FastAPI, Form, File, UploadFile, HTTPException, Request, Depends
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import uvicorn
import aiohttp
from typing import List, Optional

# –ò–º–ø–æ—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞
from src.parsers.pdf_resume_parser import PDFResumeParser
from src.parsers.vacancy_extractor import VacancyExtractor
from src.hh.api_client import HHApiClient
from src.hh.auth import HHAuthService
from src.hh.token_exchanger import HHCodeExchanger
from src.callback_local_server.config import settings as callback_settings
from src.llm_gap_analyzer.llm_gap_analyzer import LLMGapAnalyzer
from src.llm_cover_letter.llm_cover_letter_generator import EnhancedLLMCoverLetterGenerator
from src.llm_interview_checklist.llm_interview_checklist_generator import LLMInterviewChecklistGenerator
from src.llm_interview_simulation.llm_interview_simulator import ProfessionalInterviewSimulator
from src.llm_resume_rewriter.llm_resume_rewriter import LLMResumeRewriter
from src.utils import get_logger
from src.models.gap_analysis_models import EnhancedResumeTailoringAnalysis
from src.models.resume_models import ResumeInfo

# –ò–º–ø–æ—Ä—Ç —Å–∏—Å—Ç–µ–º—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
from src.security.auth import SimpleAuth
from src.security.health_dashboard import add_health_dashboard_routes

# –ò–º–ø–æ—Ä—Ç –¥–µ–º–æ-—Å–∏—Å—Ç–µ–º—ã
from src.demo_cache.demo_manager import DemoManager

# PDF –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã
from src.web_app.gap_analysis.pdf_generator import GapAnalysisPDFGenerator
from src.web_app.cover_letter.pdf_generator import CoverLetterPDFGenerator
from src.web_app.interview_checklist.pdf_generator import InterviewChecklistPDFGenerator

logger = get_logger()

def extract_profile_from_session_or_id(request_id: str, default: str = "middle") -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ—Ñ–∏–ª—è –∏–∑ ID –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ —Å–µ—Å—Å–∏–∏.
    –î–ª—è –¥–µ–º–æ-—Ä–µ–∂–∏–º–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ ID.
    """
    if not request_id:
        return default
    
    request_id_lower = request_id.lower()
    
    # –ò—â–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —É—Ä–æ–≤–Ω—è –≤ ID
    if any(pattern in request_id_lower for pattern in ['junior', '–¥–∂—É–Ω–∏–æ—Ä', '–Ω–∞—á–∏–Ω–∞—é—â–∏–π']):
        return "junior"
    elif any(pattern in request_id_lower for pattern in ['senior', '—Å–µ–Ω—å–æ—Ä', '—Å—Ç–∞—Ä—à–∏–π', 'lead']):
        return "senior"
    elif any(pattern in request_id_lower for pattern in ['middle', '–º–∏–¥–ª', '—Å—Ä–µ–¥–Ω–∏–π']):
        return "middle"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à –æ—Ç ID –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
    id_hash = hash(request_id) % 3
    levels = ["junior", "middle", "senior"]
    return levels[id_hash]

app = FastAPI(title="AI Resume Assistant - Unified Web App")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
current_dir = Path(__file__).parent
auth_system = SimpleAuth(templates_dir=str(current_dir.parent.parent / "security" / "templates"))

# –î–æ–±–∞–≤–ª—è–µ–º middleware –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
app.add_middleware(
    auth_system.get_middleware().__class__,
    config=auth_system.config,
    session_manager=auth_system.session_manager,
    templates=auth_system.templates
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤
templates = Jinja2Templates(directory=str(current_dir / "templates"))

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã)
static_dir = current_dir / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ —Å–µ—Ä–≤–∏—Å–æ–≤
pdf_parser = PDFResumeParser()
vacancy_extractor = VacancyExtractor()
hh_auth_service = HHAuthService()
token_exchanger = HHCodeExchanger()

# LLM —Å–µ—Ä–≤–∏—Å—ã
llm_gap_analyzer = LLMGapAnalyzer()
cover_letter_generator = EnhancedLLMCoverLetterGenerator(validate_quality=False)
checklist_generator = LLMInterviewChecklistGenerator()
interview_simulator = ProfessionalInterviewSimulator()
resume_rewriter = LLMResumeRewriter()

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
user_tokens = {}
analysis_storage = {}
cover_letter_storage = {}
checklist_storage = {}
simulation_storage = {}
simulation_progress_storage = {}
adapted_resume_storage = {}

# ================== –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø ==================

@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
    return await auth_system.login_page(request)

@app.post("/login")
async def login_post(request: Request, password: str = Form(...)):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
    return await auth_system.login_post(request, password)

@app.get("/logout")
async def logout(request: Request):
    """–í—ã—Ö–æ–¥ –∏–∑ —Å–∏—Å—Ç–µ–º—ã"""
    return await auth_system.logout(request)

# ================== –ì–õ–ê–í–ù–ê–Ø –°–¢–†–ê–ù–ò–¶–ê ==================

@app.get("/", response_class=HTMLResponse)
async def index(request: Request, _: bool = Depends(auth_system.require_auth)):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π –ø–æ –≤—Å–µ–º —Ñ—É–Ω–∫—Ü–∏—è–º"""
    return templates.TemplateResponse("index.html", {"request": request})

# ================== HH.RU –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø ==================

@app.post("/auth/hh")
async def start_hh_auth(_: bool = Depends(auth_system.require_auth)):
    """–ù–∞—á–∞–ª–æ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ HH.ru"""
    auth_url = hh_auth_service.get_auth_url()
    return {"auth_url": auth_url}

@app.get("/auth/tokens")
async def get_tokens_from_callback(_: bool = Depends(auth_system.require_auth)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ callback —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        if "hh_access_token" in user_tokens and "hh_refresh_token" in user_tokens:
            return {
                "success": True,
                "message": "–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"
            }
        
        callback_url = f"{callback_settings.protocol}://{callback_settings.host}:{callback_settings.port}/api/code"
        
        async with aiohttp.ClientSession() as session:
            async with session.get(callback_url) as response:
                if response.status == 200:
                    data = await response.json()
                    code = data.get("code")
                    
                    if code:
                        logger.info(f"–ü–æ–ª—É—á–µ–Ω –∫–æ–¥ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, –æ–±–º–µ–Ω–∏–≤–∞–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω—ã...")
                        
                        # –û–±–º–µ–Ω–∏–≤–∞–µ–º –∫–æ–¥ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
                        tokens = await token_exchanger.exchange_code(code)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                        user_tokens["hh_access_token"] = tokens["access_token"]
                        user_tokens["hh_refresh_token"] = tokens["refresh_token"]
                        
                        logger.info("–¢–æ–∫–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
                        
                        # –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—á–∏—â–∞–µ–º –∫–æ–¥ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
                        await session.post(f"http://{callback_settings.host}:{callback_settings.port}/api/reset_code")
                        
                        return {
                            "success": True,
                            "message": "–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞"
                        }
                    
                return {"success": False, "message": "–ö–æ–¥ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"}
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {e}")
        return {"success": False, "message": f"–û—à–∏–±–∫–∞: {str(e)}"}

# ================== –°–¢–†–ê–ù–ò–¶–´ –§–£–ù–ö–¶–ò–ô ==================

@app.get("/gap-analysis", response_class=HTMLResponse)
async def gap_analysis_page(request: Request, _: bool = Depends(auth_system.require_auth)):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞"""
    return templates.TemplateResponse("gap_analysis.html", {"request": request})

@app.get("/cover-letter", response_class=HTMLResponse)
async def cover_letter_page(request: Request, _: bool = Depends(auth_system.require_auth)):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞"""
    return templates.TemplateResponse("cover_letter.html", {"request": request})

@app.get("/interview-checklist", response_class=HTMLResponse)
async def interview_checklist_page(request: Request, _: bool = Depends(auth_system.require_auth)):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —á–µ–∫-–ª–∏—Å—Ç–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∏–Ω—Ç–µ—Ä–≤—å—é"""
    return templates.TemplateResponse("interview_checklist.html", {"request": request})

@app.get("/interview-simulation", response_class=HTMLResponse)
async def interview_simulation_page(request: Request, _: bool = Depends(auth_system.require_auth)):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
    return templates.TemplateResponse("interview_simulation.html", {"request": request})

# ================== GAP ANALYSIS ==================

@app.post("/gap-analysis")
async def perform_gap_analysis(
    resume_file: UploadFile = File(...),
    vacancy_url: str = Form(...),
    _: bool = Depends(auth_system.require_auth)
):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ"""
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        if not resume_file.filename.endswith('.pdf'):
            raise HTTPException(400, "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            content = await resume_file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ
            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ...")
            parsed_resume = pdf_parser.parse_pdf_resume(tmp_file_path)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ URL
            vacancy_id = extract_vacancy_id(vacancy_url)
            if not vacancy_id:
                raise HTTPException(400, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            if "hh_access_token" not in user_tokens or "hh_refresh_token" not in user_tokens:
                raise HTTPException(400, "–ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è HH.ru")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}...")
            hh_client = HHApiClient(user_tokens["hh_access_token"], user_tokens["hh_refresh_token"])
            vacancy_data = await hh_client.request(f'vacancies/{vacancy_id}')
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            parsed_vacancy = vacancy_extractor.extract_vacancy_info(vacancy_data)
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞
            logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞...")
            resume_dict = parsed_resume.model_dump()
            vacancy_dict = parsed_vacancy.model_dump()
            analysis_result = await llm_gap_analyzer.gap_analysis(resume_dict, vacancy_dict)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ
            analysis_id = f"gap_{hash(str(resume_dict) + str(vacancy_dict))}"
            analysis_storage[analysis_id] = {
                'analysis_result': analysis_result,
                'resume_data': resume_dict,
                'vacancy_data': vacancy_dict
            }
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            formatted_result = format_gap_analysis_for_web(analysis_result)
            
            return JSONResponse({
                "status": "success",
                "analysis": formatted_result,
                "analysis_id": analysis_id
            })
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.unlink(tmp_file_path)
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")

@app.get("/download-gap-analysis/{analysis_id}")
async def download_gap_analysis_pdf(analysis_id: str, _: bool = Depends(auth_system.require_auth)):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF –æ—Ç—á–µ—Ç–∞ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞"""
    try:
        if analysis_id not in analysis_storage:
            raise HTTPException(404, "–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        analysis_data = analysis_storage[analysis_id]
        analysis_result = analysis_data['analysis_result'] if isinstance(analysis_data, dict) else analysis_data
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF –æ—Ç—á–µ—Ç
        pdf_generator = GapAnalysisPDFGenerator()
        pdf_buffer = pdf_generator.generate_pdf(analysis_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDF –≤ —Ñ–∞–π–ª
        report_path = f"/tmp/gap_analysis_report_{analysis_id}.pdf"
        with open(report_path, 'wb') as f:
            f.write(pdf_buffer.getvalue())
        
        filename = f"Gap_Analysis_Report_{analysis_id[:8]}.pdf"
        
        return FileResponse(
            path=report_path,
            filename=filename,
            media_type='application/pdf'
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {str(e)}")

# ================== RESUME REWRITER ==================

@app.post("/adapt-resume/{analysis_id}")
async def adapt_resume(analysis_id: str, _: bool = Depends(auth_system.require_auth)):
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ GAP-–∞–Ω–∞–ª–∏–∑–∞"""
    try:
        if analysis_id not in analysis_storage:
            raise HTTPException(404, "–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        analysis_data = analysis_storage[analysis_id]
        if isinstance(analysis_data, dict):
            resume_data = analysis_data['resume_data']
            gap_analysis_data = analysis_data['analysis_result'].model_dump()
        else:
            raise HTTPException(400, "–î–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ —Ç—Ä–µ–±—É—é—Ç—Å—è –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ")
        
        logger.info(f"–ù–∞—á–∞–ª–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ {analysis_id}")
        
        # –í—ã–∑–æ–≤ LLM —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ
        adapted_resume = await resume_rewriter.rewrite_resume(resume_data, gap_analysis_data)
        
        if adapted_resume is None:
            raise HTTPException(500, "–ù–µ —É–¥–∞–ª–æ—Å—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
        adaptation_id = f"adapted_{analysis_id}_{hash(str(adapted_resume.model_dump()))}"
        adapted_resume_storage[adaptation_id] = {
            'adapted_resume': adapted_resume,
            'original_resume': resume_data,
            'gap_analysis_id': analysis_id
        }
        
        logger.info(f"–†–µ–∑—é–º–µ —É—Å–ø–µ—à–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ: {adaptation_id}")
        
        return JSONResponse({
            "status": "success",
            "message": "–†–µ–∑—é–º–µ —É—Å–ø–µ—à–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏",
            "adaptation_id": adaptation_id
        })
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ: {str(e)}")

@app.get("/download-adapted-resume/{adaptation_id}")
async def download_adapted_resume_pdf(adaptation_id: str, _: bool = Depends(auth_system.require_auth)):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ"""
    try:
        if adaptation_id not in adapted_resume_storage:
            raise HTTPException(404, "–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        adaptation_data = adapted_resume_storage[adaptation_id]
        adapted_resume = adaptation_data['adapted_resume']
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF –¥–ª—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ {adaptation_id}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF —Å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–µ–∑—é–º–µ
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ PDF –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, —á—Ç–æ –∏ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ
        from src.web_app.gap_analysis.pdf_generator import GapAnalysisPDFGenerator
        pdf_generator = GapAnalysisPDFGenerator()
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è PDF –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        temp_analysis = {
            'resume_info': adapted_resume,
            'match_percentage': 95,  # –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
            'hiring_recommendation': '–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –Ω–∞–π–º—É - —Ä–µ–∑—é–º–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏',
            'key_strengths': ['–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ', '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤–∞–∫–∞–Ω—Å–∏–∏'],
            'major_gaps': ['–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ'],
            'next_steps': '–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—é–º–µ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—é'
        }
        
        pdf_buffer = pdf_generator.generate_adapted_resume_pdf(adapted_resume)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDF –≤ —Ñ–∞–π–ª
        report_path = f"/tmp/adapted_resume_{adaptation_id}.pdf"
        with open(report_path, 'wb') as f:
            f.write(pdf_buffer.getvalue())
        
        filename = f"Adapted_Resume_{adaptation_id[:8]}.pdf"
        
        return FileResponse(
            path=report_path,
            filename=filename,
            media_type='application/pdf'
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {str(e)}")

# ================== COVER LETTER ==================

@app.post("/generate-cover-letter")
async def generate_cover_letter(
    resume_file: UploadFile = File(...),
    vacancy_url: str = Form(...),
    _: bool = Depends(auth_system.require_auth)
):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞"""
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        if not resume_file.filename.endswith('.pdf'):
            raise HTTPException(400, "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            content = await resume_file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ
            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ...")
            parsed_resume = pdf_parser.parse_pdf_resume(tmp_file_path)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ URL
            vacancy_id = extract_vacancy_id(vacancy_url)
            if not vacancy_id:
                raise HTTPException(400, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            if "hh_access_token" not in user_tokens or "hh_refresh_token" not in user_tokens:
                raise HTTPException(400, "–ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è HH.ru")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}...")
            hh_client = HHApiClient(user_tokens["hh_access_token"], user_tokens["hh_refresh_token"])
            vacancy_data = await hh_client.request(f'vacancies/{vacancy_id}')
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            parsed_vacancy = vacancy_extractor.extract_vacancy_info(vacancy_data)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞
            logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞...")
            resume_dict = parsed_resume.model_dump()
            vacancy_dict = parsed_vacancy.model_dump()
            cover_letter_result = await cover_letter_generator.generate_enhanced_cover_letter(resume_dict, vacancy_dict)
            
            if not cover_letter_result:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ")
                raise HTTPException(500, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF
            letter_id = f"letter_{hash(str(resume_dict) + str(vacancy_dict))}"
            cover_letter_storage[letter_id] = cover_letter_result
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            formatted_result = format_cover_letter_for_web(cover_letter_result)
            
            return JSONResponse({
                "status": "success",
                "cover_letter": formatted_result,
                "letter_id": letter_id
            })
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.unlink(tmp_file_path)
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

@app.get("/download-cover-letter/{letter_id}")
async def download_cover_letter_pdf(letter_id: str, _: bool = Depends(auth_system.require_auth)):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞"""
    try:
        demo_manager = DemoManager()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–º–æ-—Ä–µ–∂–∏–º
        if demo_manager.is_demo_mode():
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ—Ñ–∏–ª—è –ø–æ letter_id (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é)
            profile_level = extract_profile_from_session_or_id(letter_id, "middle")  # default fallback
            demo_pdf_path = demo_manager.get_pdf_path("cover_letter", profile_level)
            
            if demo_pdf_path and os.path.exists(demo_pdf_path):
                filename = f"Cover_Letter_{profile_level}.pdf"
                logger.info(f"üé≠ Serving demo PDF: {demo_pdf_path}")
                return FileResponse(
                    path=demo_pdf_path,
                    filename=filename,
                    media_type='application/pdf'
                )
        
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        if letter_id not in cover_letter_storage:
            raise HTTPException(404, "–°–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        cover_letter_result = cover_letter_storage[letter_id]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF –æ—Ç—á–µ—Ç
        pdf_generator = CoverLetterPDFGenerator()
        pdf_buffer = pdf_generator.generate_pdf(cover_letter_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDF –≤ —Ñ–∞–π–ª
        report_path = f"/tmp/cover_letter_report_{letter_id}.pdf"
        with open(report_path, 'wb') as f:
            f.write(pdf_buffer.getvalue())
        
        filename = f"Cover_Letter_{letter_id[:8]}.pdf"
        
        return FileResponse(
            path=report_path,
            filename=filename,
            media_type='application/pdf'
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {str(e)}")

# ================== INTERVIEW CHECKLIST ==================

@app.post("/generate-interview-checklist")
async def generate_interview_checklist(
    resume_file: UploadFile = File(...),
    vacancy_url: str = Form(...),
    _: bool = Depends(auth_system.require_auth)
):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–∫-–ª–∏—Å—Ç–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∏–Ω—Ç–µ—Ä–≤—å—é"""
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        if not resume_file.filename.endswith('.pdf'):
            raise HTTPException(400, "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            content = await resume_file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ
            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ...")
            parsed_resume = pdf_parser.parse_pdf_resume(tmp_file_path)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ URL
            vacancy_id = extract_vacancy_id(vacancy_url)
            if not vacancy_id:
                raise HTTPException(400, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            if "hh_access_token" not in user_tokens or "hh_refresh_token" not in user_tokens:
                raise HTTPException(400, "–ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è HH.ru")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}...")
            hh_client = HHApiClient(user_tokens["hh_access_token"], user_tokens["hh_refresh_token"])
            vacancy_data = await hh_client.request(f'vacancies/{vacancy_id}')
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            parsed_vacancy = vacancy_extractor.extract_vacancy_info(vacancy_data)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–∫-–ª–∏—Å—Ç–∞
            logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–∫-–ª–∏—Å—Ç–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∏–Ω—Ç–µ—Ä–≤—å—é...")
            resume_dict = parsed_resume.model_dump()
            vacancy_dict = parsed_vacancy.model_dump()
            checklist_result = await checklist_generator.generate_interview_checklist(resume_dict, vacancy_dict)
            
            if not checklist_result:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–∫-–ª–∏—Å—Ç")
                raise HTTPException(500, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–∫-–ª–∏—Å—Ç")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF
            checklist_id = f"checklist_{hash(str(resume_dict) + str(vacancy_dict))}"
            checklist_storage[checklist_id] = checklist_result
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            formatted_result = format_checklist_for_web(checklist_result)
            
            return JSONResponse({
                "status": "success",
                "checklist": formatted_result,
                "checklist_id": checklist_id
            })
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.unlink(tmp_file_path)
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ–∫-–ª–∏—Å—Ç–∞: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

@app.get("/download-interview-checklist/{checklist_id}")
async def download_interview_checklist_pdf(checklist_id: str, _: bool = Depends(auth_system.require_auth)):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF —á–µ–∫-–ª–∏—Å—Ç–∞"""
    try:
        demo_manager = DemoManager()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–º–æ-—Ä–µ–∂–∏–º
        if demo_manager.is_demo_mode():
            profile_level = extract_profile_from_session_or_id(checklist_id, "middle")
            demo_pdf_path = demo_manager.get_pdf_path("interview_checklist", profile_level)
            
            if demo_pdf_path and os.path.exists(demo_pdf_path):
                filename = f"Interview_Checklist_{profile_level}.pdf"
                logger.info(f"üé≠ Serving demo PDF: {demo_pdf_path}")
                return FileResponse(
                    path=demo_pdf_path,
                    filename=filename,
                    media_type='application/pdf'
                )
        
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        if checklist_id not in checklist_storage:
            raise HTTPException(404, "–ß–µ–∫-–ª–∏—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        checklist_result = checklist_storage[checklist_id]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF –æ—Ç—á–µ—Ç
        pdf_generator = InterviewChecklistPDFGenerator()
        pdf_buffer = pdf_generator.generate_pdf(checklist_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDF –≤ —Ñ–∞–π–ª
        report_path = f"/tmp/interview_checklist_report_{checklist_id}.pdf"
        with open(report_path, 'wb') as f:
            f.write(pdf_buffer.getvalue())
        
        filename = f"Interview_Checklist_{checklist_id[:8]}.pdf"
        
        return FileResponse(
            path=report_path,
            filename=filename,
            media_type='application/pdf'
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {str(e)}")

# ================== INTERVIEW SIMULATION ==================

@app.post("/start-interview-simulation")
async def start_interview_simulation(
    resume_file: UploadFile = File(...),
    vacancy_url: str = Form(...),
    target_rounds: int = Form(5, ge=3, le=7),
    temperature: float = Form(0.7, ge=0.1, le=1.0),
    difficulty_level: str = Form("medium"),
    hr_persona: str = Form("professional"),
    focus_areas: str = Form("[]"),  # JSON —Å—Ç—Ä–æ–∫–∞ —Å –º–∞—Å—Å–∏–≤–æ–º
    include_behavioral: bool = Form(True),
    include_technical: bool = Form(True),
    _: bool = Depends(auth_system.require_auth)
):
    """–ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        if not resume_file.filename.endswith('.pdf'):
            raise HTTPException(400, "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if difficulty_level not in ["easy", "medium", "hard"]:
            raise HTTPException(400, "–ù–µ–≤–µ—Ä–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –î–æ—Å—Ç—É–ø–Ω—ã: easy, medium, hard")
        
        if hr_persona not in ["professional", "friendly", "strict", "technical"]:
            raise HTTPException(400, "–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø HR. –î–æ—Å—Ç—É–ø–Ω—ã: professional, friendly, strict, technical")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è focus_areas JSON
        try:
            import json
            focus_areas_list = json.loads(focus_areas)
            if not isinstance(focus_areas_list, list):
                raise ValueError("focus_areas –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∞—Å—Å–∏–≤–æ–º")
            
            valid_focus_areas = ["leadership", "teamwork", "communication", "problem_solving"]
            for area in focus_areas_list:
                if area not in valid_focus_areas:
                    raise ValueError(f"–ù–µ–≤–µ—Ä–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Ñ–æ–∫—É—Å–∞: {area}")
        except (json.JSONDecodeError, ValueError) as e:
            raise HTTPException(400, f"–û—à–∏–±–∫–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–µ focus_areas: {str(e)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–æ–≤ –≤–∫–ª—é—á–µ–Ω
        if not include_behavioral and not include_technical:
            raise HTTPException(400, "–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–æ–≤ (–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ)")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            content = await resume_file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ
            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ PDF —Ä–µ–∑—é–º–µ...")
            parsed_resume = pdf_parser.parse_pdf_resume(tmp_file_path)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ URL
            vacancy_id = extract_vacancy_id(vacancy_url)
            if not vacancy_id:
                raise HTTPException(400, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            if "hh_access_token" not in user_tokens or "hh_refresh_token" not in user_tokens:
                raise HTTPException(400, "–ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è HH.ru")
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}...")
            hh_client = HHApiClient(user_tokens["hh_access_token"], user_tokens["hh_refresh_token"])
            vacancy_data = await hh_client.request(f'vacancies/{vacancy_id}')
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏
            parsed_vacancy = vacancy_extractor.extract_vacancy_info(vacancy_data)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å–∏–º—É–ª—è—Ü–∏–∏
            simulation_id = f"sim_{hash(str(parsed_resume.model_dump()) + str(parsed_vacancy.model_dump()) + str(target_rounds))}"
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            simulation_progress_storage[simulation_id] = {
                "status": "starting",
                "progress": 0,
                "message": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏..."
            }
            
            # –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
            config = {
                "target_rounds": target_rounds,
                "temperature": temperature,
                "difficulty_level": difficulty_level,
                "hr_persona": hr_persona,
                "focus_areas": focus_areas,
                "include_behavioral": include_behavioral,
                "include_technical": include_technical
            }
            
            asyncio.create_task(run_simulation_background(
                simulation_id,
                parsed_resume.model_dump(),
                parsed_vacancy.model_dump(),
                config
            ))
            
            return JSONResponse({
                "status": "started",
                "simulation_id": simulation_id,
                "message": "–°–∏–º—É–ª—è—Ü–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–ø—É—â–µ–Ω–∞"
            })
            
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.unlink(tmp_file_path)
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∏–º—É–ª—è—Ü–∏–∏: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {str(e)}")

@app.get("/simulation-progress/{simulation_id}")
async def get_simulation_progress(simulation_id: str, _: bool = Depends(auth_system.require_auth)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å–∏–º—É–ª—è—Ü–∏–∏"""
    if simulation_id not in simulation_progress_storage:
        raise HTTPException(404, "–°–∏–º—É–ª—è—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    return simulation_progress_storage[simulation_id]

@app.get("/simulation-result/{simulation_id}")
async def get_simulation_result(simulation_id: str, _: bool = Depends(auth_system.require_auth)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏"""
    if simulation_id not in simulation_storage:
        raise HTTPException(404, "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∏–º—É–ª—è—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    result = simulation_storage[simulation_id]
    formatted_result = format_simulation_for_web(result)
    
    return JSONResponse({
        "status": "success",
        "simulation": formatted_result,
        "simulation_id": simulation_id
    })

@app.get("/download-interview-simulation/{simulation_id}")
async def download_interview_simulation_pdf(simulation_id: str, _: bool = Depends(auth_system.require_auth)):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF –æ—Ç—á–µ—Ç–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
    try:
        demo_manager = DemoManager()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–º–æ-—Ä–µ–∂–∏–º
        if demo_manager.is_demo_mode():
            profile_level = extract_profile_from_session_or_id(simulation_id, "middle")
            demo_pdf_path = demo_manager.get_pdf_path("interview_simulation", profile_level)
            
            if demo_pdf_path and os.path.exists(demo_pdf_path):
                filename = f"Interview_Simulation_{profile_level}.pdf"
                logger.info(f"üé≠ Serving demo PDF: {demo_pdf_path}")
                return FileResponse(
                    path=demo_pdf_path,
                    filename=filename,
                    media_type='application/pdf'
                )
        
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        if simulation_id not in simulation_storage:
            raise HTTPException(404, "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∏–º—É–ª—è—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        simulation_result = simulation_storage[simulation_id]
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º PDF –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        from src.llm_interview_simulation.pdf_generator import ProfessionalInterviewPDFGenerator
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF –æ—Ç—á–µ—Ç
        pdf_generator = ProfessionalInterviewPDFGenerator()
        pdf_buffer = pdf_generator.generate_pdf(simulation_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDF –≤ —Ñ–∞–π–ª
        report_path = f"/tmp/interview_simulation_report_{simulation_id}.pdf"
        with open(report_path, 'wb') as f:
            f.write(pdf_buffer.getvalue())
        
        filename = f"Interview_Simulation_{simulation_id[:8]}.pdf"
        
        return FileResponse(
            path=report_path,
            filename=filename,
            media_type='application/pdf'
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF —Å–∏–º—É–ª—è—Ü–∏–∏: {e}")
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {str(e)}")

# ================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ==================

async def update_simulation_progress(simulation_id: str, round_num: int, total_rounds: int):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å–∏–º—É–ª—è—Ü–∏–∏"""
    progress = int((round_num / total_rounds) * 80) + 10  # 10-90% –¥–ª—è —Ä–∞—É–Ω–¥–æ–≤
    simulation_progress_storage[simulation_id] = {
        "status": "running",
        "progress": progress,
        "message": f"–†–∞—É–Ω–¥ {round_num}/{total_rounds}: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤..."
    }

async def run_simulation_background(simulation_id: str, resume_dict: dict, vacancy_dict: dict, config: dict):
    """–ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        simulation_progress_storage[simulation_id] = {
            "status": "running",
            "progress": 10,
            "message": "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞..."
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞
        logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {config}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π —Å–∏–º—É–ª—è—Ç–æ—Ä—É
        validated_config = {}
        if config.get("target_rounds"):
            validated_config["target_rounds"] = max(3, min(7, config["target_rounds"]))
        if config.get("temperature"):
            validated_config["temperature"] = max(0.1, min(1.0, config["temperature"]))
        if config.get("difficulty_level") in ["easy", "medium", "hard"]:
            validated_config["difficulty_level"] = config["difficulty_level"]
        if config.get("hr_persona") in ["professional", "friendly", "strict", "technical"]:
            validated_config["hr_persona"] = config["hr_persona"]
        
        # –ü–∞—Ä—Å–∏–º focus_areas –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ JSON
        if config.get("focus_areas"):
            try:
                import json
                if isinstance(config["focus_areas"], str):
                    validated_config["focus_areas"] = json.loads(config["focus_areas"])
                else:
                    validated_config["focus_areas"] = config["focus_areas"]
            except (json.JSONDecodeError, TypeError):
                logger.warning("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ focus_areas, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
                validated_config["focus_areas"] = []
        
        validated_config["include_behavioral"] = config.get("include_behavioral", True)
        validated_config["include_technical"] = config.get("include_technical", True)
        
        interview_simulator.set_custom_config(validated_config)
        
        simulation_progress_storage[simulation_id] = {
            "status": "running",
            "progress": 30,
            "message": "–ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é..."
        }
        
        # –°–æ–∑–¥–∞–µ–º async progress callback
        async def progress_callback(round_num: int, total_rounds: int):
            logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å —Å–∏–º—É–ª—è—Ü–∏–∏: {round_num}/{total_rounds}")
            await update_simulation_progress(simulation_id, round_num, total_rounds)
        
        # –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏
        logger.info("–ó–∞–ø—É—Å–∫ –º–µ—Ç–æ–¥–∞ simulate_interview...")
        simulation_result = await interview_simulator.simulate_interview(
            resume_dict, 
            vacancy_dict,
            progress_callback=progress_callback,
            config_overrides=config
        )
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∏–º—É–ª—è—Ü–∏–∏: {type(simulation_result)}")
        
        simulation_progress_storage[simulation_id] = {
            "status": "running",
            "progress": 90,
            "message": "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–∏–º—É–ª—è—Ü–∏–∏..."
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∏–º—É–ª—è—Ü–∏–∏
        if simulation_result is None:
            raise Exception("–°–∏–º—É–ª—è—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–∏—á–∏–Ω—ã: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ/–≤–∞–∫–∞–Ω—Å–∏–∏, –ø—Ä–æ–±–ª–µ–º—ã —Å OpenAI API, –∏–ª–∏ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–∏–º—É–ª—è—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if not hasattr(simulation_result, 'dialog_messages') or not simulation_result.dialog_messages:
            raise Exception("–°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –±–µ–∑ –¥–∏–∞–ª–æ–≥–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–∏.")
        
        if not hasattr(simulation_result, 'assessment') or not simulation_result.assessment:
            logger.warning("–°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –±–µ–∑ –æ—Ü–µ–Ω–∫–∏, –Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∏–∞–ª–æ–≥ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ.")
        
        logger.info(f"–°–∏–º—É–ª—è—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(simulation_result.dialog_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π, –æ—Ü–µ–Ω–∫–∞: {hasattr(simulation_result, 'assessment') and simulation_result.assessment is not None}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        simulation_storage[simulation_id] = simulation_result
        
        simulation_progress_storage[simulation_id] = {
            "status": "completed",
            "progress": 100,
            "message": "–°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏: {e}")
        simulation_progress_storage[simulation_id] = {
            "status": "error",
            "progress": 0,
            "message": f"–û—à–∏–±–∫–∞: {str(e)}"
        }

def extract_vacancy_id(vacancy_url: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ URL"""
    import re
    pattern = r'https?://(?:(?:www\.|[a-z]+\.)?)?hh\.ru/vacancy/(\d+)'
    match = re.search(pattern, vacancy_url)
    return match.group(1) if match else None

def format_gap_analysis_for_web(analysis: EnhancedResumeTailoringAnalysis) -> dict:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥–∞–ø-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    return {
        "primary_screening": {
            "overall_result": analysis.primary_screening.overall_screening_result,
            "job_title_match": analysis.primary_screening.job_title_match,
            "experience_match": analysis.primary_screening.experience_years_match,
            "skills_visible": analysis.primary_screening.key_skills_visible,
            "location_suitable": analysis.primary_screening.location_suitable,
            "salary_match": analysis.primary_screening.salary_expectations_match,
            "notes": analysis.primary_screening.screening_notes
        },
        "requirements_analysis": [
            {
                "requirement_text": req.requirement_text,
                "requirement_type": req.requirement_type,
                "skill_category": req.skill_category,
                "compliance_status": req.compliance_status,
                "evidence_in_resume": req.evidence_in_resume,
                "gap_description": req.gap_description,
                "impact_on_decision": req.impact_on_decision
            } for req in analysis.requirements_analysis
        ],
        "quality_assessment": {
            "structure_clarity": analysis.quality_assessment.structure_clarity,
            "content_relevance": analysis.quality_assessment.content_relevance,
            "achievement_focus": analysis.quality_assessment.achievement_focus,
            "adaptation_quality": analysis.quality_assessment.adaptation_quality,
            "overall_impression": analysis.quality_assessment.overall_impression,
            "quality_notes": analysis.quality_assessment.quality_notes
        },
        "critical_recommendations": [
            {
                "section": rec.section,
                "criticality": rec.criticality,
                "issue_description": rec.issue_description,
                "specific_actions": rec.specific_actions,
                "example_wording": rec.example_wording,
                "business_rationale": rec.business_rationale
            } for rec in analysis.critical_recommendations
        ],
        "important_recommendations": [
            {
                "section": rec.section,
                "criticality": rec.criticality,
                "issue_description": rec.issue_description,
                "specific_actions": rec.specific_actions,
                "example_wording": rec.example_wording,
                "business_rationale": rec.business_rationale
            } for rec in analysis.important_recommendations
        ],
        "optional_recommendations": [
            {
                "section": rec.section,
                "criticality": rec.criticality,
                "issue_description": rec.issue_description,
                "specific_actions": rec.specific_actions,
                "example_wording": rec.example_wording,
                "business_rationale": rec.business_rationale
            } for rec in analysis.optional_recommendations
        ],
        "match_percentage": analysis.overall_match_percentage,
        "hiring_recommendation": analysis.hiring_recommendation,
        "key_strengths": analysis.key_strengths,
        "major_gaps": analysis.major_gaps,
        "next_steps": analysis.next_steps
    }

def format_cover_letter_for_web(cover_letter) -> dict:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    return {
        "company_context": {
            "company_name": cover_letter.company_context.company_name,
            "position_title": cover_letter.subject_line,
            "company_size": cover_letter.company_context.company_size,
            "role_type": cover_letter.role_type,
            "key_requirements": [cover_letter.skills_match.relevant_experience]
        },
        "skills_match": {
            "matching_skills": cover_letter.skills_match.matched_skills,
            "relevant_achievements": [cover_letter.skills_match.quantified_achievement] if cover_letter.skills_match.quantified_achievement else [],
            "unique_selling_points": [cover_letter.personalization.value_proposition],
            "experience_relevance_score": cover_letter.relevance_score
        },
        "personalization": {
            "tone": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π",
            "key_motivations": [cover_letter.personalization.role_motivation],
            "company_research_points": [cover_letter.personalization.company_knowledge] if cover_letter.personalization.company_knowledge else [],
            "customization_level": "–≤—ã—Å–æ–∫–∏–π" if cover_letter.personalization_score >= 8 else "—Å—Ä–µ–¥–Ω–∏–π"
        },
        "letter_structure": {
            "opening_hook": cover_letter.opening_hook,
            "value_proposition": cover_letter.personalization.value_proposition,
            "specific_examples": [cover_letter.relevant_experience],
            "company_alignment": cover_letter.company_interest,
            "call_to_action": cover_letter.professional_closing
        },
        "final_letter": assemble_full_letter_text(cover_letter),
        "quality_assessment": {
            "personalization_score": cover_letter.personalization_score,
            "relevance_score": cover_letter.relevance_score,
            "engagement_score": cover_letter.professional_tone_score,
            "overall_quality": "EXCELLENT" if cover_letter.personalization_score >= 8 else "GOOD" if cover_letter.personalization_score >= 6 else "AVERAGE",
            "improvement_suggestions": cover_letter.improvement_suggestions
        }
    }

def assemble_full_letter_text(cover_letter) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–∏—Å—å–º–∞ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    parts = [
        f"–¢–µ–º–∞: {cover_letter.subject_line}",
        "",
        cover_letter.personalized_greeting,
        "",
        cover_letter.opening_hook,
        "",
        cover_letter.company_interest,
        "",
        cover_letter.relevant_experience,
        "",
        cover_letter.value_demonstration,
    ]
    
    if cover_letter.growth_mindset:
        parts.extend(["", cover_letter.growth_mindset])
    
    parts.extend([
        "",
        cover_letter.professional_closing,
        "",
        cover_letter.signature
    ])
    
    return "\n".join(parts)

def format_checklist_for_web(checklist) -> dict:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ–∫-–ª–∏—Å—Ç–∞ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–ª–∏ —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è
    if hasattr(checklist, 'executive_summary'):
        # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
        return {
            "type": "professional",
            "candidate_level": checklist.personalization_context.candidate_level,
            "vacancy_type": checklist.personalization_context.vacancy_type,
            "company_format": checklist.personalization_context.company_format,
            "executive_summary": {
                "preparation_strategy": checklist.preparation_strategy,
                "key_focus_areas": checklist.personalization_context.critical_focus_areas,
                "estimated_prep_time": checklist.time_estimates.total_time_needed,
                "priority_recommendations": checklist.personalization_context.key_gaps_identified
            },
            "technical_preparation": [
                {
                    "category": item.category,
                    "priority": item.priority,
                    "tasks": [item.task_title, item.description],
                    "time_estimate": item.estimated_time,
                    "resources": item.specific_resources
                } for item in checklist.technical_preparation
            ],
            "behavioral_preparation": [
                {
                    "category": item.category,
                    "priority": "–í–ê–ñ–ù–û",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏
                    "tasks": [item.task_title, item.description] + item.example_questions,
                    "time_estimate": "1-2 —á–∞—Å–∞",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "resources": [item.practice_tips]
                } for item in checklist.behavioral_preparation
            ],
            "company_research": [
                {
                    "category": item.category,
                    "priority": item.priority,
                    "tasks": [item.task_title] + item.specific_actions,
                    "time_estimate": item.time_required,
                    "resources": []
                } for item in checklist.company_research
            ],
            "technical_stack_study": [
                {
                    "category": item.category,
                    "priority": "–í–ê–ñ–ù–û",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "tasks": [item.task_title, item.description, item.study_approach],
                    "time_estimate": "2-4 —á–∞—Å–∞",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "resources": []
                } for item in checklist.technical_stack_study
            ],
            "practical_exercises": [
                {
                    "category": item.category,
                    "priority": "–í–ê–ñ–ù–û",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "tasks": [item.exercise_title, item.description, f"–£—Ä–æ–≤–µ–Ω—å: {item.difficulty_level}"],
                    "time_estimate": "3-5 —á–∞—Å–æ–≤",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "resources": item.practice_resources
                } for item in checklist.practical_exercises
            ],
            "interview_setup": [
                {
                    "category": item.category,
                    "priority": "–ö–†–ò–¢–ò–ß–ù–û",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                    "tasks": [item.task_title] + item.checklist_items,
                    "time_estimate": "30 –º–∏–Ω—É—Ç",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "resources": []
                } for item in checklist.interview_setup
            ],
            "additional_actions": [
                {
                    "category": item.category,
                    "priority": item.urgency,
                    "tasks": [item.action_title, item.description] + item.implementation_steps,
                    "time_estimate": "1-2 —á–∞—Å–∞",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    "resources": []
                } for item in checklist.additional_actions
            ],
            "critical_success_factors": checklist.critical_success_factors
        }
    else:
        # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
        return {
            "type": "basic",
            "technical_preparation": [
                {
                    "category": "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è_–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞",
                    "priority": skill.priority,
                    "tasks": [skill.skill_name, skill.study_plan, f"–¢–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å: {skill.current_level_assessment}"],
                    "time_estimate": getattr(skill, 'estimated_time', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                    "resources": [r.title for r in skill.resources]
                } for skill in checklist.technical_skills
            ],
            "behavioral_preparation": [
                {
                    "category": q.question_category,
                    "priority": "–í–ê–ñ–ù–û",
                    "tasks": q.example_questions + [q.preparation_tips],
                    "time_estimate": "1-2 —á–∞—Å–∞",
                    "resources": [q.star_method_examples] if q.star_method_examples else []
                } for q in checklist.behavioral_questions
            ],
            "company_research": [
                {
                    "category": "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏",
                    "priority": "–í–ê–ñ–ù–û",
                    "tasks": [checklist.company_research_tips],
                    "time_estimate": "2-3 —á–∞—Å–∞",
                    "resources": []
                }
            ],
            "general_recommendations": [checklist.final_recommendations] if hasattr(checklist, 'final_recommendations') else []
        }

def format_simulation_for_web(simulation) -> dict:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–ª—è –≤–µ–±-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if simulation is None:
        return {
            "interview_rounds": [],
            "final_assessment": {
                "overall_score": 0,
                "strengths": ["–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"],
                "areas_for_improvement": ["–°–∏–º—É–ª—è—Ü–∏—è –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"],
                "hiring_recommendation": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
                "detailed_feedback": "–°–∏–º—É–ª—è—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
            },
            "summary": {
                "total_rounds": 0,
                "average_score": 0,
                "simulation_duration": "–ø—Ä–µ—Ä–≤–∞–Ω–∞",
                "key_insights": ["–°–∏–º—É–ª—è—Ü–∏—è –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"]
            }
        }
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º dialog_messages –≤ —Ä–∞—É–Ω–¥—ã –∏–Ω—Ç–µ—Ä–≤—å—é
    interview_rounds = []
    hr_messages = [msg for msg in simulation.dialog_messages if msg.speaker == "HR"]
    candidate_messages = [msg for msg in simulation.dialog_messages if msg.speaker == "Candidate"]
    
    for hr_msg in hr_messages:
        round_num = hr_msg.round_number
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –æ—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        candidate_msg = next(
            (msg for msg in candidate_messages if msg.round_number == round_num), 
            None
        )
        
        interview_rounds.append({
            "round_number": round_num,
            "question": hr_msg.message,
            "candidate_response": candidate_msg.message if candidate_msg else "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞",
            "hr_feedback": "",  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â—É—é—Å—è –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
            "score": candidate_msg.response_quality if candidate_msg and candidate_msg.response_quality else 0,
            "improvement_notes": ""  # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        })
    
    # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏
    scores = [r["score"] for r in interview_rounds if r["score"] > 0]
    average_score = sum(scores) / len(scores) if scores else 0
    
    return {
        "interview_rounds": interview_rounds,
        "final_assessment": {
            "overall_score": int(average_score * 2),  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —à–∫–∞–ª–µ 1-10
            "strengths": simulation.assessment.strengths if simulation.assessment else ["–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"],
            "areas_for_improvement": simulation.assessment.weaknesses if simulation.assessment else ["–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"],
            "hiring_recommendation": simulation.assessment.overall_recommendation if simulation.assessment else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
            "detailed_feedback": simulation.hr_assessment if hasattr(simulation, 'hr_assessment') else "–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
        },
        "summary": {
            "total_rounds": len(interview_rounds),
            "average_score": round(average_score, 1),
            "simulation_duration": "–∑–∞–≤–µ—Ä—à–µ–Ω–∞",
            "key_insights": simulation.assessment.weaknesses[:3] if simulation.assessment and simulation.assessment.weaknesses else ["–ù–µ—Ç –∏–Ω—Å–∞–π—Ç–æ–≤"]
        }
    }

# ================== –ú–û–ù–ò–¢–û–†–ò–ù–ì ==================

@app.get("/health")
async def health_check():
    """Health check endpoint –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    try:
        return {
            "status": "healthy",
            "service": "ai-resume-assistant-unified",
            "version": "1.0.0",
            "port": 3000,
            "checks": {
                "auth_system": "ok",
                "templates": "ok",
                "storage": "ok"
            }
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "service": "ai-resume-assistant-unified",
            "error": str(e)
        }

# –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä—à—Ä—É—Ç—ã –ø–∞–Ω–µ–ª–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
add_health_dashboard_routes(app, auth_system.templates)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=3000)