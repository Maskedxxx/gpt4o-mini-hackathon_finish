# src/tg_bot/handlers/gap_analyzer_handler.py
from aiogram import types
from aiogram.fsm.context import FSMContext
from langsmith import traceable, Client
import os

from src.tg_bot.utils import UserState
from src.tg_bot.utils import GAP_ANALYZE_MESSAGES
from src.tg_bot.utils import authorized_keyboard
from src.llm_gap_analyzer import LLMGapAnalyzer
from src.models.gap_analysis_models import EnhancedResumeTailoringAnalysis

from src.utils import get_logger
logger = get_logger()

# ===============================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –õ–ò–ú–ò–¢–û–í –î–õ–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê
# ===============================================

# –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ª–∏–º–∏—Ç—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
DISPLAY_LIMITS = {
    # –õ–∏–º–∏—Ç—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    'max_requirements_per_group': 3,      # –ú–∞–∫—Å–∏–º—É–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ (MUST/NICE/BONUS)
    'max_recommendations_per_group': 3,   # –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ (CRITICAL/IMPORTANT/OPTIONAL)
    'max_strengths_display': 3,           # –ú–∞–∫—Å–∏–º—É–º —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    'max_gaps_display': 3,                # –ú–∞–∫—Å–∏–º—É–º –ø—Ä–æ–±–µ–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    
    # –õ–∏–º–∏—Ç—ã –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
    'requirement_text_length': 60,        # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    'gap_description_length': 80,         # –î–ª–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞
    'example_wording_length': 80,         # –î–ª–∏–Ω–∞ –ø—Ä–∏–º–µ—Ä–∞ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
    'recommendation_issue_length': 100,   # –î–ª–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    
    # –õ–∏–º–∏—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    'progress_bar_width': 10,             # –®–∏—Ä–∏–Ω–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    'score_bar_width': 10,                # –®–∏—Ä–∏–Ω–∞ –±–∞—Ä–∞ –æ—Ü–µ–Ω–∫–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
}

# –°–∏–º–≤–æ–ª—ã –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
DISPLAY_SYMBOLS = {
    'progress_filled': '‚ñì',               # –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –±–ª–æ–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    'progress_empty': '‚ñë',                # –ü—É—Å—Ç–æ–π –±–ª–æ–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    'score_filled': '‚ñì',                  # –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –±–ª–æ–∫ –æ—Ü–µ–Ω–∫–∏
    'score_empty': '‚ñë',                   # –ü—É—Å—Ç–æ–π –±–ª–æ–∫ –æ—Ü–µ–Ω–∫–∏
    'ellipsis': '...',                    # –ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ –¥–ª—è –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
}

# ===============================================

# –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç LangSmith
def create_langsmith_client():
    api_key = os.getenv("LANGCHAIN_API_KEY")
    if not api_key:
        return print("LANGCHAIN_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —Ç—Ä–µ–π—Å–∏–Ω–≥ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω")
    return Client(api_key=api_key)

ls_client = create_langsmith_client()

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
llm_analyzer = LLMGapAnalyzer()

def truncate_text(text: str, max_length: int) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è."""
    if len(text) <= max_length:
        return text
    return text[:max_length] + DISPLAY_SYMBOLS['ellipsis']

def format_primary_screening(analysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞."""
    screening = analysis.primary_screening
    
    # –≠–º–æ–¥–∑–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    def bool_emoji(value: bool) -> str:
        return "‚úÖ" if value else "‚ùå"
    
    result_emoji = {
        "PASS": "‚úÖ",
        "MAYBE": "‚ö†Ô∏è", 
        "REJECT": "‚ùå"
    }
    
    result = "üìã –ü–ï–†–í–ò–ß–ù–´–ô –°–ö–†–ò–ù–ò–ù–ì (7-15 —Å–µ–∫—É–Ω–¥)\n\n"
    
    result += f"{bool_emoji(screening.job_title_match)} –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏: {'–î–∞' if screening.job_title_match else '–ù–µ—Ç'}\n"
    result += f"{bool_emoji(screening.experience_years_match)} –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–∞–∂–∞: {'–î–∞' if screening.experience_years_match else '–ù–µ—Ç'}\n"
    result += f"{bool_emoji(screening.key_skills_visible)} –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –≤–∏–¥–Ω—ã: {'–î–∞' if screening.key_skills_visible else '–ù–µ—Ç'}\n"
    result += f"{bool_emoji(screening.location_suitable)} –õ–æ–∫–∞—Ü–∏—è –ø–æ–¥—Ö–æ–¥–∏—Ç: {'–î–∞' if screening.location_suitable else '–ù–µ—Ç'}\n"
    result += f"{bool_emoji(screening.salary_expectations_match)} –ó–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è: {'–°–æ–≤–ø–∞–¥–∞—é—Ç' if screening.salary_expectations_match else '–ù–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç'}\n\n"
    
    result += f"{result_emoji.get(screening.overall_screening_result, '‚ùì')} –ò–¢–û–ì –°–ö–†–ò–ù–ò–ù–ì–ê: {screening.overall_screening_result}\n\n"
    result += f"üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:\n{screening.screening_notes}"
    
    return result

def format_requirements_analysis(analysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π."""
    if not analysis.requirements_analysis:
        return ""
    
    result = "üîç –ê–ù–ê–õ–ò–ó –¢–†–ï–ë–û–í–ê–ù–ò–ô –í–ê–ö–ê–ù–°–ò–ò\n\n"
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    must_have = [r for r in analysis.requirements_analysis if r.requirement_type == "MUST_HAVE"]
    nice_to_have = [r for r in analysis.requirements_analysis if r.requirement_type == "NICE_TO_HAVE"]
    bonus = [r for r in analysis.requirements_analysis if r.requirement_type == "BONUS"]
    
    def format_requirement_group(requirements, title, emoji):
        if not requirements:
            return ""
        
        group_result = f"{emoji} {title}\n"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        max_items = DISPLAY_LIMITS['max_requirements_per_group']
        for req in requirements[:max_items]:
            status_emoji = {
                "–ü–û–õ–ù–û–ï_–°–û–û–¢–í–ï–¢–°–¢–í–ò–ï": "‚úÖ",
                "–ß–ê–°–¢–ò–ß–ù–û–ï_–°–û–û–¢–í–ï–¢–°–¢–í–ò–ï": "‚ö†Ô∏è",
                "–û–¢–°–£–¢–°–¢–í–£–ï–¢": "‚ùå",
                "–¢–†–ï–ë–£–ï–¢_–£–¢–û–ß–ù–ï–ù–ò–Ø": "üîç"
            }
            
            emoji_status = status_emoji.get(req.compliance_status, "‚ùì")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            requirement_text = truncate_text(req.requirement_text, DISPLAY_LIMITS['requirement_text_length'])
            group_result += f"  {emoji_status} {requirement_text}\n"
            
            if req.gap_description and req.compliance_status != "–ü–û–õ–ù–û–ï_–°–û–û–¢–í–ï–¢–°–¢–í–ò–ï":
                gap_text = truncate_text(req.gap_description, DISPLAY_LIMITS['gap_description_length'])
                group_result += f"     üí° {gap_text}\n"
        
        if len(requirements) > max_items:
            group_result += f"     {DISPLAY_SYMBOLS['ellipsis']} –∏ –µ—â–µ {len(requirements) - max_items} —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π\n"
        
        return group_result + "\n"
    
    result += format_requirement_group(must_have, "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø", "üî¥")
    result += format_requirement_group(nice_to_have, "–ñ–ï–õ–ê–¢–ï–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø", "üü°")
    result += format_requirement_group(bonus, "–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–õ–Æ–°–´", "üü¢")
    
    return result

def format_quality_assessment(analysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—é–º–µ."""
    quality = analysis.quality_assessment
    
    def score_bar(score: int) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø–æ–ª–æ—Å–∫—É –æ—Ü–µ–Ω–∫–∏."""
        bar_width = DISPLAY_LIMITS['score_bar_width']
        filled = DISPLAY_SYMBOLS['score_filled'] * score
        empty = DISPLAY_SYMBOLS['score_empty'] * (bar_width - score)
        return f"{filled}{empty} {score}/{bar_width}"
    
    impression_emoji = {
        "STRONG": "üî•",
        "AVERAGE": "üëç",
        "WEAK": "üëé"
    }
    
    result = "üìä –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –†–ï–ó–Æ–ú–ï\n\n"
    
    result += f"üèó –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å:\n{score_bar(quality.structure_clarity)}\n\n"
    result += f"üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:\n{score_bar(quality.content_relevance)}\n\n"
    result += f"üèÜ –§–æ–∫—É—Å –Ω–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:\n{score_bar(quality.achievement_focus)}\n\n"
    result += f"üîß –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –≤–∞–∫–∞–Ω—Å–∏—é:\n{score_bar(quality.adaptation_quality)}\n\n"
    
    emoji = impression_emoji.get(quality.overall_impression, "‚ùì")
    result += f"{emoji} –û–ë–©–ï–ï –í–ü–ï–ß–ê–¢–õ–ï–ù–ò–ï: {quality.overall_impression}\n\n"
    result += f"üí¨ {quality.quality_notes}"
    
    return result

def format_recommendations(analysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é."""
    result = "üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ\n\n"
    
    def format_recommendation_group(recommendations, title, emoji):
        if not recommendations:
            return ""
        
        group_result = f"{emoji} {title}\n"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        max_items = DISPLAY_LIMITS['max_recommendations_per_group']
        for i, rec in enumerate(recommendations[:max_items], 1):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã
            issue_text = truncate_text(rec.issue_description, DISPLAY_LIMITS['recommendation_issue_length'])
            group_result += f"{i}. {rec.section.upper()}: {issue_text}\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            if rec.specific_actions:
                group_result += f"   üìù {rec.specific_actions[0]}\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –µ—Å—Ç—å (—Å –ª–∏–º–∏—Ç–æ–º)
            if rec.example_wording:
                example = truncate_text(rec.example_wording, DISPLAY_LIMITS['example_wording_length'])
                group_result += f"   üí° –ü—Ä–∏–º–µ—Ä: {example}\n"
            
            group_result += "\n"
        
        if len(recommendations) > max_items:
            group_result += f"{DISPLAY_SYMBOLS['ellipsis']} –∏ –µ—â–µ {len(recommendations) - max_items} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n\n"
        
        return group_result
    
    result += format_recommendation_group(analysis.critical_recommendations, "–ö–†–ò–¢–ò–ß–ù–´–ï (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)", "üî¥")
    result += format_recommendation_group(analysis.important_recommendations, "–í–ê–ñ–ù–´–ï", "üü°")
    result += format_recommendation_group(analysis.optional_recommendations, "–ñ–ï–õ–ê–¢–ï–õ–¨–ù–´–ï", "üü¢")
    
    return result

def format_final_conclusion(analysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã."""
    hiring_emoji = {
        "STRONG_YES": "üî•",
        "YES": "‚úÖ",
        "MAYBE": "ü§î",
        "NO": "‚ùå", 
        "STRONG_NO": "üö´"
    }
    
    result = "üéØ –ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´\n\n"
    
    # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
    percentage = analysis.overall_match_percentage
    bar_width = DISPLAY_LIMITS['progress_bar_width']
    filled_blocks = percentage // (100 // bar_width)
    filled_char = DISPLAY_SYMBOLS['progress_filled']
    empty_char = DISPLAY_SYMBOLS['progress_empty']
    progress_bar = filled_char * filled_blocks + empty_char * (bar_width - filled_blocks)
    result += f"üìä –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏: {progress_bar} {percentage}%\n\n"
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –Ω–∞–π–º—É
    emoji = hiring_emoji.get(analysis.hiring_recommendation, "‚ùì")
    result += f"{emoji} –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø –ü–û –ù–ê–ô–ú–£: {analysis.hiring_recommendation}\n\n"
    
    # –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã (—Å –ª–∏–º–∏—Ç–æ–º)
    if analysis.key_strengths:
        result += "üí™ –ö–õ–Æ–ß–ï–í–´–ï –°–ò–õ–¨–ù–´–ï –°–¢–û–†–û–ù–´:\n"
        max_strengths = DISPLAY_LIMITS['max_strengths_display']
        for strength in analysis.key_strengths[:max_strengths]:
            result += f"‚Ä¢ {strength}\n"
        if len(analysis.key_strengths) > max_strengths:
            result += f"‚Ä¢ {DISPLAY_SYMBOLS['ellipsis']} –∏ –µ—â–µ {len(analysis.key_strengths) - max_strengths}\n"
        result += "\n"
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã (—Å –ª–∏–º–∏—Ç–æ–º)
    if analysis.major_gaps:
        result += "‚ö†Ô∏è –û–°–ù–û–í–ù–´–ï –ü–†–û–ë–ï–õ–´:\n"
        max_gaps = DISPLAY_LIMITS['max_gaps_display']
        for gap in analysis.major_gaps[:max_gaps]:
            result += f"‚Ä¢ {gap}\n"
        if len(analysis.major_gaps) > max_gaps:
            result += f"‚Ä¢ {DISPLAY_SYMBOLS['ellipsis']} –∏ –µ—â–µ {len(analysis.major_gaps) - max_gaps}\n"
        result += "\n"
    
    # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    result += f"üë£ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:\n{analysis.next_steps}"
    
    return result

def format_enhanced_gap_analysis_preview(analysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä –∞–Ω–∞–ª–∏–∑–∞."""
    result = "üìä –†–ê–°–®–ò–†–ï–ù–ù–´–ô GAP-–ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù\n\n"
    
    screening_result = analysis.primary_screening.overall_screening_result
    screening_emoji = {"PASS": "‚úÖ", "MAYBE": "‚ö†Ô∏è", "REJECT": "‚ùå"}
    
    result += f"{screening_emoji.get(screening_result, '‚ùì')} –°–∫—Ä–∏–Ω–∏–Ω–≥: {screening_result}\n"
    result += f"üìä –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: {analysis.overall_match_percentage}%\n"
    result += f"üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {analysis.hiring_recommendation}\n\n"
    
    result += f"üî¥ –ö—Ä–∏—Ç–∏—á–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(analysis.critical_recommendations)}\n"
    result += f"üü° –í–∞–∂–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(analysis.important_recommendations)}\n"
    result += f"üü¢ –ñ–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π: {len(analysis.optional_recommendations)}\n\n"
    
    result += "üì± –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –ø–æ —á–∞—Å—Ç—è–º..."
    
    return result

@traceable(client=ls_client, project_name="llamaindex_test", run_type = "retriever")
async def start_gap_analysis(message: types.Message, state: FSMContext):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ gap-–∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ."""
    user_id = message.from_user.id
    logger.info(f"–ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ gap-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_data = await state.get_data()
    parsed_resume = user_data.get("parsed_resume")
    parsed_vacancy = user_data.get("parsed_vacancy")
    
    if not parsed_resume or not parsed_vacancy:
        logger.error(f"parser_error –¥–ª—è {user_id}")
        await message.answer(GAP_ANALYZE_MESSAGES['analysis_error'])
        await state.set_state(UserState.AUTHORIZED)
        return
    
    # –°–æ–æ–±—â–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ –Ω–∞—á–∞–ª–µ –∞–Ω–∞–ª–∏–∑–∞
    await message.answer(GAP_ANALYZE_MESSAGES["analysis_started"], reply_markup=authorized_keyboard)
    await state.set_state(UserState.RESUME_GAP_ANALYZE)
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π gap-–∞–Ω–∞–ª–∏–∑
        gap_analysis_result = await llm_analyzer.gap_analysis(parsed_resume, parsed_vacancy)
        
        if not gap_analysis_result:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π gap-–∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            await message.answer(GAP_ANALYZE_MESSAGES["analysis_error"])
            await state.set_state(UserState.AUTHORIZED)
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        await state.update_data(gap_analysis=gap_analysis_result.model_dump())
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —á–∞—Å—Ç—è–º
        await send_enhanced_gap_analysis_in_parts(message, gap_analysis_result)
        
        await state.set_state(UserState.AUTHORIZED)
        logger.info(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π gap-–∞–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ gap-–∞–Ω–∞–ª–∏–∑–∞: {e}")
        await message.answer(GAP_ANALYZE_MESSAGES["analysis_error"])
        await state.set_state(UserState.AUTHORIZED)

async def send_enhanced_gap_analysis_in_parts(message: types.Message, analysis: EnhancedResumeTailoringAnalysis):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —á–∞—Å—Ç—è–º."""
    
    # –ß–∞—Å—Ç—å 1: –ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä
    preview = format_enhanced_gap_analysis_preview(analysis)
    await message.answer(preview)
    
    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
    import asyncio
    await asyncio.sleep(1)
    
    # –ß–∞—Å—Ç—å 2: –ü–µ—Ä–≤–∏—á–Ω—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥
    screening = format_primary_screening(analysis)
    await message.answer(screening)
    
    await asyncio.sleep(1)
    
    # –ß–∞—Å—Ç—å 3: –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    requirements = format_requirements_analysis(analysis)
    if requirements:
        await message.answer(requirements)
        await asyncio.sleep(1)
    
    # –ß–∞—Å—Ç—å 4: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    quality = format_quality_assessment(analysis)
    await message.answer(quality)
    
    await asyncio.sleep(1)
    
    # –ß–∞—Å—Ç—å 5: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    recommendations = format_recommendations(analysis)
    await message.answer(recommendations)
    
    await asyncio.sleep(1)
    
    # –ß–∞—Å—Ç—å 6: –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã (—Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä–æ–π)
    conclusion = format_final_conclusion(analysis)
    await message.answer(conclusion, reply_markup=authorized_keyboard)